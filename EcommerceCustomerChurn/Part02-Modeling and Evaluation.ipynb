{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Part02-Modeling and Evaluation.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1s0kaWSL5I1J31s39Yt_z9SPymvRp12z5","authorship_tag":"ABX9TyM8yInOwbnJmx3DJ/bAg3v3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"24xvSwcoGNqM"},"source":["!pip install category_encoders &> /dev/null"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VddokEgKDt3V"},"source":["!pip install imblearn &> /dev/null"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dErU3RHKGM2m","executionInfo":{"status":"ok","timestamp":1638205165236,"user_tz":-420,"elapsed":18,"user":{"displayName":"Rendi Salim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08689642810800137700"}},"outputId":"8368a1f8-9e8a-4456-ba99-63d3ea4af43f"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"eNwg9tAGAUDN"},"source":["# Part02 - Modeling: <p>\n","\n","<h2> Introduction: </h2> <p>\n","Basically, modeling is a process of data mining, because by making a model we can find patterns from customers who will churn and those who will not. In this notebook, we will create appropriate model that can predict our customer will be churn or not. The key word is \"appropriate\" means not only the model with the highest accuracy, but the model that we will create must be interpreted as to why it can take a churn decision and our model must also not be biased. As I said in previous notebook, we will try some machine learning model algorithms:\n","<ol>\n","<li> Logistic Regression</li>\n","<li> KNN</li>\n","<li> Decision Tree</li>\n","<li> Random Forest</li>\n","<li> XGBoost</li> \n","</ol><p>\n","In the modeling process, we have to do some data preprocessing, such as:\n","<ul>\n","<li> Scaling</li>\n","<li> Encoding</li>\n","<li> SMOTE</li>\n","<li> Polynomial</li>\n","</ul><p>\n","At the end, we will evaluate to get the appropriate model and we will save the model so that we can deploy later.\n"]},{"cell_type":"markdown","metadata":{"id":"a1S9Azt2A07E"},"source":["## Load Library:"]},{"cell_type":"code","metadata":{"id":"3iT5oHiZ5fpU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638205166718,"user_tz":-420,"elapsed":1485,"user":{"displayName":"Rendi Salim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08689642810800137700"}},"outputId":"781c5868-ec99-472b-ee41-4fc799d8669f"},"source":["# Data\n","import pandas as pd\n","import numpy as np\n","\n","# Visualization\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Analyze\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","\n","# Model Preprocessing\n","from imblearn.over_sampling import SMOTE\n","from imblearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.preprocessing import OneHotEncoder\n","from category_encoders import OrdinalEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.decomposition import PCA\n","\n","# Model Evaluasi\n","from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, cross_val_score\n","from sklearn.metrics import classification_report, f1_score, plot_roc_curve\n","\n","# Warnings\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"]}]},{"cell_type":"markdown","metadata":{"id":"2epgkUikG9o7"},"source":["## Load Clean Dataset:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":257},"id":"MGIR2_cQDB-r","executionInfo":{"status":"ok","timestamp":1638205167437,"user_tz":-420,"elapsed":722,"user":{"displayName":"Rendi Salim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08689642810800137700"}},"outputId":"6e7ccc0e-203e-4751-c385-0cecfee5f313"},"source":["df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/EcommerceCustomerChurn/CleanDataset.csv')\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Churn</th>\n","      <th>Tenure</th>\n","      <th>PreferredLoginDevice</th>\n","      <th>CityTier</th>\n","      <th>WarehouseToHome</th>\n","      <th>PreferredPaymentMode</th>\n","      <th>Gender</th>\n","      <th>HourSpendOnApp</th>\n","      <th>NumberOfDeviceRegistered</th>\n","      <th>PreferedOrderCat</th>\n","      <th>MaritalStatus</th>\n","      <th>NumberOfAddress</th>\n","      <th>Complain</th>\n","      <th>OrderAmountHikeFromlastYear</th>\n","      <th>CouponUsed</th>\n","      <th>OrderCount</th>\n","      <th>DaySinceLastOrder</th>\n","      <th>CashbackAmount</th>\n","      <th>Satisfaction</th>\n","      <th>AvgCashbackPerOrder</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>4.0</td>\n","      <td>Mobile Phone</td>\n","      <td>3</td>\n","      <td>6.0</td>\n","      <td>Debit Card</td>\n","      <td>Female</td>\n","      <td>3.0</td>\n","      <td>3</td>\n","      <td>Laptop &amp; Accessory</td>\n","      <td>Single</td>\n","      <td>9</td>\n","      <td>1</td>\n","      <td>11.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>159.93</td>\n","      <td>Unhappy</td>\n","      <td>159.93</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>9.0</td>\n","      <td>Mobile Phone</td>\n","      <td>1</td>\n","      <td>8.0</td>\n","      <td>UPI</td>\n","      <td>Male</td>\n","      <td>3.0</td>\n","      <td>4</td>\n","      <td>Mobile Phone</td>\n","      <td>Single</td>\n","      <td>7</td>\n","      <td>1</td>\n","      <td>15.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>120.90</td>\n","      <td>Neutral</td>\n","      <td>120.90</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>9.0</td>\n","      <td>Mobile Phone</td>\n","      <td>1</td>\n","      <td>30.0</td>\n","      <td>Debit Card</td>\n","      <td>Male</td>\n","      <td>2.0</td>\n","      <td>4</td>\n","      <td>Mobile Phone</td>\n","      <td>Single</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>14.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>120.28</td>\n","      <td>Neutral</td>\n","      <td>120.28</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>Mobile Phone</td>\n","      <td>3</td>\n","      <td>15.0</td>\n","      <td>Debit Card</td>\n","      <td>Male</td>\n","      <td>2.0</td>\n","      <td>4</td>\n","      <td>Laptop &amp; Accessory</td>\n","      <td>Single</td>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>23.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>134.07</td>\n","      <td>Happy</td>\n","      <td>134.07</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>Mobile Phone</td>\n","      <td>1</td>\n","      <td>12.0</td>\n","      <td>Credit Card</td>\n","      <td>Male</td>\n","      <td>3.0</td>\n","      <td>3</td>\n","      <td>Mobile Phone</td>\n","      <td>Single</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>11.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>129.60</td>\n","      <td>Happy</td>\n","      <td>129.60</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Churn  Tenure  ... Satisfaction  AvgCashbackPerOrder\n","0      1     4.0  ...      Unhappy               159.93\n","1      1     9.0  ...      Neutral               120.90\n","2      1     9.0  ...      Neutral               120.28\n","3      1     0.0  ...        Happy               134.07\n","4      1     0.0  ...        Happy               129.60\n","\n","[5 rows x 20 columns]"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ukj4IlgrHaYf","executionInfo":{"status":"ok","timestamp":1638205167438,"user_tz":-420,"elapsed":50,"user":{"displayName":"Rendi Salim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08689642810800137700"}},"outputId":"c87cb197-5c5a-412d-cb8f-fdcf29f22526"},"source":["df.info()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 5630 entries, 0 to 5629\n","Data columns (total 20 columns):\n"," #   Column                       Non-Null Count  Dtype  \n","---  ------                       --------------  -----  \n"," 0   Churn                        5630 non-null   int64  \n"," 1   Tenure                       5630 non-null   float64\n"," 2   PreferredLoginDevice         5630 non-null   object \n"," 3   CityTier                     5630 non-null   int64  \n"," 4   WarehouseToHome              5630 non-null   float64\n"," 5   PreferredPaymentMode         5630 non-null   object \n"," 6   Gender                       5630 non-null   object \n"," 7   HourSpendOnApp               5630 non-null   float64\n"," 8   NumberOfDeviceRegistered     5630 non-null   int64  \n"," 9   PreferedOrderCat             5630 non-null   object \n"," 10  MaritalStatus                5630 non-null   object \n"," 11  NumberOfAddress              5630 non-null   int64  \n"," 12  Complain                     5630 non-null   int64  \n"," 13  OrderAmountHikeFromlastYear  5630 non-null   float64\n"," 14  CouponUsed                   5630 non-null   float64\n"," 15  OrderCount                   5630 non-null   float64\n"," 16  DaySinceLastOrder            5630 non-null   float64\n"," 17  CashbackAmount               5630 non-null   float64\n"," 18  Satisfaction                 5630 non-null   object \n"," 19  AvgCashbackPerOrder          5630 non-null   float64\n","dtypes: float64(9), int64(5), object(6)\n","memory usage: 879.8+ KB\n"]}]},{"cell_type":"markdown","metadata":{"id":"qQfIwwHJLJkV"},"source":["## Drop Bias Columns:\n","As I said before, we don't want to create a biased model. For that, we need to remove the columns that could cause our model to be biased or tend to favor certain categories. On our dataset, there are <strong>Gender</strong> and <strong>MaritalStatus</strong> features."]},{"cell_type":"code","metadata":{"id":"c2sZvuNLKMnZ"},"source":["df.drop(columns=['Gender', 'MaritalStatus'], inplace = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gVsnaMuULuRd","executionInfo":{"status":"ok","timestamp":1638205167440,"user_tz":-420,"elapsed":39,"user":{"displayName":"Rendi Salim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08689642810800137700"}},"outputId":"5493d998-ea06-42a6-c82d-3ef5278961bb"},"source":["df.columns"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['Churn', 'Tenure', 'PreferredLoginDevice', 'CityTier',\n","       'WarehouseToHome', 'PreferredPaymentMode', 'HourSpendOnApp',\n","       'NumberOfDeviceRegistered', 'PreferedOrderCat', 'NumberOfAddress',\n","       'Complain', 'OrderAmountHikeFromlastYear', 'CouponUsed', 'OrderCount',\n","       'DaySinceLastOrder', 'CashbackAmount', 'Satisfaction',\n","       'AvgCashbackPerOrder'],\n","      dtype='object')"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tPicTV6PLvWg","executionInfo":{"status":"ok","timestamp":1638205167445,"user_tz":-420,"elapsed":41,"user":{"displayName":"Rendi Salim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08689642810800137700"}},"outputId":"9f5ddd0d-1933-4d91-ea58-8d9dfec9c347"},"source":["df.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5630, 18)"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"B9ZqRYdHNxHe"},"source":["## Handling Multicolinearity:\n","In previous notebook, we found that our data has a multicollinearity problem, especially on <strong>HourSpendOnApp</strong>, <strong>NumberOfDeviceRegistered</strong>, <strong>OrderAmountHikeFromlastYear</strong>, <strong>CashbackAmount</strong>, and probably <strong>AvgCashbackPerOrder</strong> features. Basically, multicolinearity doesn't affect the prediction result of tree-based model. Multicolinearity causes the results of the regression model to be unstable because the independent feature will also affect the other independent features. To understand more about the impact of multicollinearity on linear models, <a href = 'https://medium.com/analytics-vidhya/what-is-multicollinearity-and-how-to-remove-it-413c419de2f'>read here</a>. <p>\n","In the tree-based model, multicollinearity does not have an impact on the prediction results because this model uses the splitter concept, if you want to know more about tree-based model, you can <a href = 'https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html'>read here</a>. However, if we want to identify the most important features in predicting customer churn, the results might be wrong because these important features are correlated with other features so that the importance of these features is low <sup>[1]</sup>. To avoid that mistake, we will handle multicolinearity by dropping the feature that has the highest VIF value. <p>\n","Basically, we can use PCA to handle our multicolinearity problem. Dropping the feature that has the highest VIF value have the risk of losing important information about the pattern of customer churn and by using PCA we can avoid that risk. However, remember our key word, \"appropriate\", the disadvantage of using PCA is that it has low interpretability, if we decide to use the PCA method, we will have difficulty explaining our model. <p>\n","Okay, let's handle our multicolinearity problem."]},{"cell_type":"code","metadata":{"id":"Y85QNJEoNtxr","colab":{"base_uri":"https://localhost:8080/","height":450},"executionInfo":{"status":"ok","timestamp":1638205181541,"user_tz":-420,"elapsed":374,"user":{"displayName":"Rendi Salim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08689642810800137700"}},"outputId":"86087944-c417-4b97-946e-4d88784a1473"},"source":["numFeat = df.select_dtypes(include=np.number).columns.tolist()\n","\n","## Check multicolinearity\n","def calc_vif(x):\n","  vif = pd.DataFrame()\n","  vif['variable'] = x.columns\n","  vif['vif'] = [variance_inflation_factor(x.values, i) for i in range(x.shape[1])]\n","\n","  return vif\n","\n","calc_vif(df[numFeat[1:]])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>variable</th>\n","      <th>vif</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Tenure</td>\n","      <td>3.397104</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>CityTier</td>\n","      <td>4.171916</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>WarehouseToHome</td>\n","      <td>4.585438</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>HourSpendOnApp</td>\n","      <td>19.102331</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>NumberOfDeviceRegistered</td>\n","      <td>14.825241</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>NumberOfAddress</td>\n","      <td>4.086215</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Complain</td>\n","      <td>1.398222</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>OrderAmountHikeFromlastYear</td>\n","      <td>15.675112</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>CouponUsed</td>\n","      <td>3.280053</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>OrderCount</td>\n","      <td>6.523937</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>DaySinceLastOrder</td>\n","      <td>3.438659</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>CashbackAmount</td>\n","      <td>27.865451</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>AvgCashbackPerOrder</td>\n","      <td>9.376981</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                       variable        vif\n","0                        Tenure   3.397104\n","1                      CityTier   4.171916\n","2               WarehouseToHome   4.585438\n","3                HourSpendOnApp  19.102331\n","4      NumberOfDeviceRegistered  14.825241\n","5               NumberOfAddress   4.086215\n","6                      Complain   1.398222\n","7   OrderAmountHikeFromlastYear  15.675112\n","8                    CouponUsed   3.280053\n","9                    OrderCount   6.523937\n","10            DaySinceLastOrder   3.438659\n","11               CashbackAmount  27.865451\n","12          AvgCashbackPerOrder   9.376981"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"zh93Aimx2-n-"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"sOa5Dn5lkuqy"},"source":["# Source:\n","<sup>[1]</sup> https://medium.com/@manepriyanka48/multicollinearity-in-tree-based-models-b971292db140 <p>"]},{"cell_type":"code","metadata":{"id":"fiNAzcVwk07a"},"source":[""],"execution_count":null,"outputs":[]}]}